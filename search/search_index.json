{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pytest-elk-reporter A plugin to send pytest test results to ELK stack, with extra context data Features Reporting into Elasticsearch each test result, as the test finish Automaticlly append context data to each test: git inforamtion such as branch or last commit and more all of Jenkins env variables username if available Report a test summery to Elastic for each session with all the context data can append any user data into the context sent to elastic Requirements having pytest tests written Installation You can install \"pytest-elk-reporter\" via pip from PyPI pip install pytest-elk-reporter ElasticSearch configureation We need this auto_create_index enable for the indexes that are going to be used, since we don't have code to create the indexes, this is the default curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"action.auto_create_index\": \"true\" } } ' For more info on this elasticsearch feature check thier index documention Usage Run and configure from command line pytest --es-address 127 .0.0.1:9200 # or if you need user/password to authenticate pytest --es-address my-elk-server.io:9200 --es-username fruch --es-password 'passwordsarenicetohave' Configure from code (Idealy in conftest.py) import pytest @pytest . fixture ( scope = 'session' , autouse = True ) def configure_es ( elk_reporter ): # TODO: get cerdentials in more secure fashion programtically, maybe AWS secrects or the likes # or put them in plain-text in the code... what can ever go wrong... elk_reporter . es_address = \"my-elk-server.io:9200\" elk_reporter . es_user = 'fruch' elk_reporter . es_password = 'passwordsarenicetohave' elk_reporter . es_index_name = 'test_data' Configure from pytest ini file # put this in pytest.ini / tox.ini / setup.cfg [pytest] es_address = my-elk-server.io:9200 es_user = fruch es_password = passwordsarenicetohave es_index_name = test_data see pytest docs for more about how to configure using .ini files Collect context data for the whole session For example, with this I'll be able to build a dash board per version @pytest . fixture ( scope = \"session\" , autouse = True ) def report_formal_version_to_elk ( request ): \"\"\" Append my own data specific, for example which of the code uner test is used \"\"\" # TODO: take it programticly of of the code under test... my_data = { \"formal_version\" : \"1.0.0-rc2\" } elk = request . config . pluginmanager . get_plugin ( \"elk-reporter-runtime\" ) elk . session_data . update ( ** my_data ) Collect data for specific tests def test_my_service_and_collect_timings ( request , elk_reporter ): response = requests . get ( \"http://my-server.io/api/do_something\" ) assert response . status_code == 200 elk_reporter . append_test_data ( request , { \"do_something_response_time\" : response . elapsed . total_seconds () }) # now doing response time per version dashboard quite easy # and yeah, it's not exactly real usable metric, it's just an example... Contributing Contributions are very welcome. Tests can be run with tox , please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"pytest-elk-reporter\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description. Thanks This pytest plugin was generated with Cookiecutter along with @hackebrot 's cookiecutter-pytest-plugin template.","title":"pytest-elk-reporter"},{"location":"#pytest-elk-reporter","text":"A plugin to send pytest test results to ELK stack, with extra context data","title":"pytest-elk-reporter"},{"location":"#features","text":"Reporting into Elasticsearch each test result, as the test finish Automaticlly append context data to each test: git inforamtion such as branch or last commit and more all of Jenkins env variables username if available Report a test summery to Elastic for each session with all the context data can append any user data into the context sent to elastic","title":"Features"},{"location":"#requirements","text":"having pytest tests written","title":"Requirements"},{"location":"#installation","text":"You can install \"pytest-elk-reporter\" via pip from PyPI pip install pytest-elk-reporter","title":"Installation"},{"location":"#elasticsearch-configureation","text":"We need this auto_create_index enable for the indexes that are going to be used, since we don't have code to create the indexes, this is the default curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"action.auto_create_index\": \"true\" } } ' For more info on this elasticsearch feature check thier index documention","title":"ElasticSearch configureation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#run-and-configure-from-command-line","text":"pytest --es-address 127 .0.0.1:9200 # or if you need user/password to authenticate pytest --es-address my-elk-server.io:9200 --es-username fruch --es-password 'passwordsarenicetohave'","title":"Run and configure from command line"},{"location":"#configure-from-code-idealy-in-conftestpy","text":"import pytest @pytest . fixture ( scope = 'session' , autouse = True ) def configure_es ( elk_reporter ): # TODO: get cerdentials in more secure fashion programtically, maybe AWS secrects or the likes # or put them in plain-text in the code... what can ever go wrong... elk_reporter . es_address = \"my-elk-server.io:9200\" elk_reporter . es_user = 'fruch' elk_reporter . es_password = 'passwordsarenicetohave' elk_reporter . es_index_name = 'test_data'","title":"Configure from code (Idealy in conftest.py)"},{"location":"#configure-from-pytest-ini-file","text":"# put this in pytest.ini / tox.ini / setup.cfg [pytest] es_address = my-elk-server.io:9200 es_user = fruch es_password = passwordsarenicetohave es_index_name = test_data see pytest docs for more about how to configure using .ini files","title":"Configure from pytest ini file"},{"location":"#collect-context-data-for-the-whole-session","text":"For example, with this I'll be able to build a dash board per version @pytest . fixture ( scope = \"session\" , autouse = True ) def report_formal_version_to_elk ( request ): \"\"\" Append my own data specific, for example which of the code uner test is used \"\"\" # TODO: take it programticly of of the code under test... my_data = { \"formal_version\" : \"1.0.0-rc2\" } elk = request . config . pluginmanager . get_plugin ( \"elk-reporter-runtime\" ) elk . session_data . update ( ** my_data )","title":"Collect context data for the whole session"},{"location":"#collect-data-for-specific-tests","text":"def test_my_service_and_collect_timings ( request , elk_reporter ): response = requests . get ( \"http://my-server.io/api/do_something\" ) assert response . status_code == 200 elk_reporter . append_test_data ( request , { \"do_something_response_time\" : response . elapsed . total_seconds () }) # now doing response time per version dashboard quite easy # and yeah, it's not exactly real usable metric, it's just an example...","title":"Collect data for specific tests"},{"location":"#contributing","text":"Contributions are very welcome. Tests can be run with tox , please ensure the coverage at least stays the same before you submit a pull request.","title":"Contributing"},{"location":"#license","text":"Distributed under the terms of the MIT license, \"pytest-elk-reporter\" is free and open source software","title":"License"},{"location":"#issues","text":"If you encounter any problems, please file an issue along with a detailed description.","title":"Issues"},{"location":"#thanks","text":"This pytest plugin was generated with Cookiecutter along with @hackebrot 's cookiecutter-pytest-plugin template.","title":"Thanks"}]}